{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35fb0141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AdamW\n",
    "import pickle\n",
    "import torch\n",
    "def defaultdict_to_dict(d):\n",
    "    \"\"\" Recursively convert defaultdict to dict. \"\"\"\n",
    "    if isinstance(d, defaultdict):\n",
    "        d = {key: defaultdict_to_dict(value) for key, value in d.items()}\n",
    "    return d\n",
    "\n",
    "\n",
    "\n",
    "# Reading data and models\n",
    "groups = [str(i) for i in range(1,3)]\n",
    "peers = [str(i) for i in range(8090, 8100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "786849ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_first_files_with_str(directory, str_contain, index):\n",
    "    # Create an empty list to store files that contain str_contain\n",
    "    files_with_str = []\n",
    "\n",
    "    # Iterate over the files in the directory\n",
    "    for file in os.listdir(directory):\n",
    "        if file[:4] == str_contain:\n",
    "            files_with_str.append(file)\n",
    "    \n",
    "    \n",
    "    # Sort only the files that contain 'x1'\n",
    "    files_with_str.sort()\n",
    "\n",
    "    # Return the first file in the sorted list, or None if the list is empty\n",
    "    return files_with_str[index] if files_with_str else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0865bbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got here 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 8090 8090_2024-04-11 030343_my_t5_model\n",
      "1 8091 8091_2024-04-11 030426_my_t5_model\n",
      "1 8092 8092_2024-04-11 030433_my_t5_model\n",
      "1 8093 8093_2024-04-11 030454_my_t5_model\n",
      "1 8094 8094_2024-04-11 030414_my_t5_model\n",
      "1 8095 8095_2024-04-11 030430_my_t5_model\n",
      "1 8096 8096_2024-04-11 030557_my_t5_model\n",
      "1 8097 8097_2024-04-11 030346_my_t5_model\n",
      "1 8098 8098_2024-04-11 030508_my_t5_model\n",
      "1 8099 8099_2024-04-11 030421_my_t5_model\n",
      "2 8090 8090_2024-04-16 023258_my_t5_model\n",
      "2 8091 8091_2024-04-16 023213_my_t5_model\n",
      "2 8092 8092_2024-04-16 023255_my_t5_model\n",
      "2 8093 8093_2024-04-16 023133_my_t5_model\n",
      "2 8094 8094_2024-04-16 023012_my_t5_model\n",
      "2 8095 8095_2024-04-16 023120_my_t5_model\n",
      "2 8096 8096_2024-04-16 023233_my_t5_model\n",
      "2 8097 8097_2024-04-16 023434_my_t5_model\n",
      "2 8098 8098_2024-04-16 023210_my_t5_model\n",
      "2 8099 8099_2024-04-16 023015_my_t5_model\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "\n",
    "#reading entire dataset for all groups: train_df_group1\n",
    "\n",
    "\n",
    "# reading individual peer datasets & group datasets: train_df_group1, train_df_group1_peer1\n",
    "for group in groups:\n",
    "    # creating train_df's\n",
    "    exec(f'train_df_group{group} = pd.DataFrame()')\n",
    "    for peer in peers:\n",
    "        datasets_folder = os.path.join('aggregated_results',f'group{group}','datasets')\n",
    "        exec_str = f\"train_df_group{group}_peer{int(peer) - 8089} = pd.read_csv(os.path.join(datasets_folder,'{peer}_df.csv'))\"\n",
    "        exec(exec_str)\n",
    "        exec(f'train_df_group{group} = pd.concat([train_df_group{group}, train_df_group{group}_peer{int(peer) - 8089}])')\n",
    "       \n",
    "    exec(f'train_df_group{group} = train_df_group{group}.drop_duplicates()')\n",
    "    \n",
    "    # creating test_df's: test_df_group1, test_df_group1_peer1\n",
    "    \n",
    "    datasets_folder = os.path.join('aggregated_results',f'group{group}','datasets')\n",
    "    exec (f\"test_df_group{group} = pd.read_csv(os.path.join(datasets_folder,'test_df.csv')) \")\n",
    "    exec (f\"test_df_group{group} = test_df_group{group}[test_df_group{group}['doc_id'].isin(train_df_group{group}['doc_id'].unique())]\")\n",
    "    for peer in peers:\n",
    "        exec (f\"test_df_group{group}_peer{int(peer) - 8089} = test_df_group{group}[test_df_group{group}['doc_id'].isin(train_df_group{group}_peer{int(peer) - 8089}['doc_id'].unique())]\")\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#reading models: model_group1_peer1\n",
    "for group in groups:\n",
    "    for peer in peers:\n",
    "        model_folder = os.path.join('aggregated_results',f'group{group}', 'models')\n",
    "        model_file = find_first_files_with_str(model_folder, peer, 4) # 10 is the largest number of saved models that all peers have finished training\n",
    "        print (group, peer, model_file)\n",
    "        exec_str = f\"model_group{group}_peer{str(int(peer)-8089)} = T5ForConditionalGeneration.from_pretrained(os.path.join(model_folder, model_file))\"\n",
    "        \n",
    "        \n",
    "        exec(exec_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8a65048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_group1_peer1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d55c8c6",
   "metadata": {},
   "source": [
    "### CALCULATING ACCURACIES ON MODELS' OWN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fde6e06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_model_and_evaluate(group, peer, mode = 'global'):\n",
    "    global_scope = globals()\n",
    "    acc_train = -1\n",
    "    print ('group ', group, 'peer ', peer)\n",
    "    exec (f'model = model_group{group}_peer{peer}', global_scope)\n",
    "    if mode == 'global':\n",
    "        exec (f'train_df = train_df_group{group}.sample(1000).copy()', global_scope)\n",
    "        exec (f'test_df = test_df_group{group}.sample(1000).copy()', global_scope)\n",
    "    elif mode == 'local':\n",
    "        exec (f'train_df = train_df_group{group}_peer{peer}.sample(1000).copy()', global_scope)\n",
    "        exec (f'test_df = test_df_group{group}_peer{peer}.sample(1000).copy()', global_scope)\n",
    "        \n",
    "    df_tot = train_df.copy()\n",
    "    df_tst = test_df.copy()\n",
    "    print (df_tot.shape, df_tst.shape)\n",
    "    \n",
    "    df_tot['generated_doc_id'] = df_tot['query'].apply(lambda x: generate_text(x, model))\n",
    "    df_tst['generated_doc_id'] = df_tst['query'].apply(lambda x: generate_text(x, model))\n",
    "    acc_train = df_tot[df_tot['doc_id'] == df_tot['generated_doc_id']].shape[0]/df_tot.shape[0]\n",
    "    acc_test = df_tst[df_tst['doc_id'] == df_tst['generated_doc_id']].shape[0]/df_tst.shape[0]\n",
    "    \n",
    "    \n",
    "    print (f'{mode} training set accuracy: ', acc_train)\n",
    "    print (f'{mode} test set accuracy: ', acc_test)\n",
    "    \n",
    "    \n",
    "    df_tot['generated_doc_id_log'] = df_tot['query'].apply(lambda x: generate_text_through_logits(x, model, df_tot))\n",
    "    df_tst['generated_doc_id_log'] = df_tst['query'].apply(lambda x: generate_text_through_logits(x, model, df_tst))\n",
    "\n",
    "    \n",
    "    acc_train_log = df_tot[df_tot['doc_id'] == df_tot['generated_doc_id_log']].shape[0]/df_tot.shape[0]\n",
    "    acc_test_log = df_tst[df_tst['doc_id'] == df_tst['generated_doc_id_log']].shape[0]/df_tst.shape[0]\n",
    "    \n",
    "    print (f'{mode} training set accuracy log: ', acc_train_log)\n",
    "    print (f'{mode} test set accuracy log: ', acc_test_log)\n",
    "    return acc_train, acc_test, df_tot, df_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9bc08ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text\n",
    "def generate_text(query, model):\n",
    "    input_ids = tokenizer.encode(query, return_tensors='pt')\n",
    "    output = model.generate(input_ids, max_length = 20)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe58e0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_through_logits(query,model, df_tst):\n",
    "    doc_id = df_tst[df_tst['query'] == query]['doc_id'].iloc[0]\n",
    "#     print (query, doc_id)\n",
    "    inputs = tokenizer(query, padding=False, return_tensors=\"pt\", truncation=True).input_ids\n",
    "    labels = tokenizer(doc_id, padding=True, return_tensors=\"pt\", truncation=True).input_ids\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(input_ids=inputs, labels = labels)\n",
    "    loss = outputs.loss\n",
    "\n",
    "    # Extract logits and convert to token IDs\n",
    "    logits = outputs.logits\n",
    "    predicted_token_ids = tokenizer.decode(torch.argmax(logits, dim=-1)[0], skip_special_tokens=True)\n",
    "#     print (predicted_token_ids)\n",
    "    return predicted_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "328a3ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group  1 peer  1\n",
      "(1000, 5) (1000, 5)\n",
      "local training set accuracy:  0.999\n",
      "local test set accuracy:  0.903\n",
      "local training set accuracy log:  0.999\n",
      "local test set accuracy log:  0.903\n",
      "group  1 peer  1\n",
      "(1000, 5) (1000, 5)\n",
      "global training set accuracy:  0.995\n",
      "global test set accuracy:  0.887\n",
      "global training set accuracy log:  0.995\n",
      "global test set accuracy log:  0.887\n",
      "group  1 peer  2\n",
      "(1000, 5) (1000, 5)\n",
      "local training set accuracy:  0.99\n",
      "local test set accuracy:  0.886\n",
      "local training set accuracy log:  0.99\n",
      "local test set accuracy log:  0.886\n",
      "group  1 peer  2\n",
      "(1000, 5) (1000, 5)\n",
      "global training set accuracy:  0.988\n",
      "global test set accuracy:  0.887\n",
      "global training set accuracy log:  0.988\n",
      "global test set accuracy log:  0.887\n",
      "group  1 peer  3\n",
      "(1000, 5) (1000, 5)\n",
      "local training set accuracy:  0.995\n",
      "local test set accuracy:  0.911\n",
      "local training set accuracy log:  0.995\n",
      "local test set accuracy log:  0.911\n",
      "group  1 peer  3\n",
      "(1000, 5) (1000, 5)\n",
      "global training set accuracy:  0.993\n",
      "global test set accuracy:  0.911\n",
      "global training set accuracy log:  0.993\n",
      "global test set accuracy log:  0.911\n",
      "group  1 peer  4\n",
      "(1000, 5) (1000, 5)\n",
      "local training set accuracy:  0.991\n",
      "local test set accuracy:  0.899\n",
      "local training set accuracy log:  0.991\n",
      "local test set accuracy log:  0.899\n",
      "group  1 peer  4\n",
      "(1000, 5) (1000, 5)\n",
      "global training set accuracy:  0.991\n",
      "global test set accuracy:  0.892\n",
      "global training set accuracy log:  0.991\n",
      "global test set accuracy log:  0.892\n",
      "group  1 peer  5\n",
      "(1000, 5) (1000, 5)\n",
      "local training set accuracy:  0.992\n",
      "local test set accuracy:  0.878\n",
      "local training set accuracy log:  0.992\n",
      "local test set accuracy log:  0.878\n",
      "group  1 peer  5\n",
      "(1000, 5) (1000, 5)\n",
      "global training set accuracy:  0.991\n",
      "global test set accuracy:  0.893\n",
      "global training set accuracy log:  0.991\n",
      "global test set accuracy log:  0.893\n",
      "group  1 peer  6\n",
      "(1000, 5) (1000, 5)\n",
      "local training set accuracy:  0.991\n",
      "local test set accuracy:  0.887\n",
      "local training set accuracy log:  0.991\n",
      "local test set accuracy log:  0.887\n",
      "group  1 peer  6\n",
      "(1000, 5) (1000, 5)\n",
      "global training set accuracy:  0.993\n",
      "global test set accuracy:  0.886\n",
      "global training set accuracy log:  0.993\n",
      "global test set accuracy log:  0.886\n",
      "group  1 peer  7\n",
      "(1000, 5) (1000, 5)\n",
      "local training set accuracy:  1.0\n",
      "local test set accuracy:  0.892\n",
      "local training set accuracy log:  1.0\n",
      "local test set accuracy log:  0.892\n",
      "group  1 peer  7\n",
      "(1000, 5) (1000, 5)\n",
      "global training set accuracy:  0.993\n",
      "global test set accuracy:  0.892\n",
      "global training set accuracy log:  0.993\n",
      "global test set accuracy log:  0.892\n",
      "group  1 peer  8\n",
      "(1000, 5) (1000, 5)\n",
      "local training set accuracy:  0.995\n",
      "local test set accuracy:  0.904\n",
      "local training set accuracy log:  0.995\n",
      "local test set accuracy log:  0.904\n",
      "group  1 peer  8\n",
      "(1000, 5) (1000, 5)\n",
      "global training set accuracy:  0.996\n",
      "global test set accuracy:  0.885\n",
      "global training set accuracy log:  0.996\n",
      "global test set accuracy log:  0.885\n",
      "group  1 peer  9\n",
      "(1000, 5) (1000, 5)\n",
      "local training set accuracy:  0.99\n",
      "local test set accuracy:  0.874\n",
      "local training set accuracy log:  0.99\n",
      "local test set accuracy log:  0.874\n",
      "group  1 peer  9\n",
      "(1000, 5) (1000, 5)\n",
      "global training set accuracy:  0.99\n",
      "global test set accuracy:  0.91\n",
      "global training set accuracy log:  0.989\n",
      "global test set accuracy log:  0.91\n",
      "group  1 peer  10\n",
      "(1000, 5) (1000, 5)\n",
      "local training set accuracy:  1.0\n",
      "local test set accuracy:  0.894\n",
      "local training set accuracy log:  1.0\n",
      "local test set accuracy log:  0.894\n",
      "group  1 peer  10\n",
      "(1000, 5) (1000, 5)\n",
      "global training set accuracy:  0.996\n",
      "global test set accuracy:  0.882\n",
      "global training set accuracy log:  0.996\n",
      "global test set accuracy log:  0.882\n",
      "group  2 peer  1\n",
      "(1000, 5) (1000, 5)\n",
      "local training set accuracy:  0.993\n",
      "local test set accuracy:  0.912\n",
      "local training set accuracy log:  0.993\n",
      "local test set accuracy log:  0.912\n",
      "group  2 peer  1\n",
      "(1000, 5) (1000, 5)\n",
      "global training set accuracy:  0.993\n",
      "global test set accuracy:  0.892\n",
      "global training set accuracy log:  0.993\n",
      "global test set accuracy log:  0.892\n",
      "group  2 peer  2\n",
      "(1000, 5) (1000, 5)\n",
      "local training set accuracy:  0.986\n",
      "local test set accuracy:  0.891\n",
      "local training set accuracy log:  0.983\n",
      "local test set accuracy log:  0.887\n",
      "group  2 peer  2\n",
      "(1000, 5) (1000, 5)\n",
      "global training set accuracy:  0.992\n",
      "global test set accuracy:  0.9\n",
      "global training set accuracy log:  0.992\n",
      "global test set accuracy log:  0.9\n",
      "group  2 peer  3\n",
      "(1000, 5) (1000, 5)\n",
      "local training set accuracy:  0.993\n",
      "local test set accuracy:  0.903\n",
      "local training set accuracy log:  0.993\n",
      "local test set accuracy log:  0.903\n",
      "group  2 peer  3\n",
      "(1000, 5) (1000, 5)\n",
      "global training set accuracy:  0.992\n",
      "global test set accuracy:  0.908\n",
      "global training set accuracy log:  0.992\n",
      "global test set accuracy log:  0.908\n",
      "group  2 peer  4\n",
      "(1000, 5) (1000, 5)\n",
      "local training set accuracy:  0.991\n",
      "local test set accuracy:  0.922\n",
      "local training set accuracy log:  0.991\n",
      "local test set accuracy log:  0.922\n",
      "group  2 peer  4\n",
      "(1000, 5) (1000, 5)\n",
      "global training set accuracy:  0.988\n",
      "global test set accuracy:  0.904\n",
      "global training set accuracy log:  0.988\n",
      "global test set accuracy log:  0.904\n",
      "group  2 peer  5\n",
      "(1000, 5) (1000, 5)\n",
      "local training set accuracy:  0.992\n",
      "local test set accuracy:  0.91\n",
      "local training set accuracy log:  0.992\n",
      "local test set accuracy log:  0.91\n",
      "group  2 peer  5\n",
      "(1000, 5) (1000, 5)\n",
      "global training set accuracy:  0.991\n",
      "global test set accuracy:  0.895\n",
      "global training set accuracy log:  0.991\n",
      "global test set accuracy log:  0.895\n",
      "group  2 peer  6\n",
      "(1000, 5) (1000, 5)\n",
      "local training set accuracy:  0.982\n",
      "local test set accuracy:  0.921\n",
      "local training set accuracy log:  0.982\n",
      "local test set accuracy log:  0.921\n",
      "group  2 peer  6\n",
      "(1000, 5) (1000, 5)\n",
      "global training set accuracy:  0.993\n",
      "global test set accuracy:  0.906\n",
      "global training set accuracy log:  0.993\n",
      "global test set accuracy log:  0.906\n",
      "group  2 peer  7\n",
      "(1000, 5) (1000, 5)\n",
      "local training set accuracy:  0.988\n",
      "local test set accuracy:  0.876\n",
      "local training set accuracy log:  0.988\n",
      "local test set accuracy log:  0.876\n",
      "group  2 peer  7\n",
      "(1000, 5) (1000, 5)\n",
      "global training set accuracy:  0.994\n",
      "global test set accuracy:  0.883\n",
      "global training set accuracy log:  0.994\n",
      "global test set accuracy log:  0.883\n",
      "group  2 peer  8\n",
      "(1000, 5) (1000, 5)\n",
      "local training set accuracy:  0.991\n",
      "local test set accuracy:  0.896\n",
      "local training set accuracy log:  0.991\n",
      "local test set accuracy log:  0.896\n",
      "group  2 peer  8\n",
      "(1000, 5) (1000, 5)\n",
      "global training set accuracy:  0.991\n",
      "global test set accuracy:  0.894\n",
      "global training set accuracy log:  0.991\n",
      "global test set accuracy log:  0.894\n",
      "group  2 peer  9\n",
      "(1000, 5) (1000, 5)\n",
      "local training set accuracy:  0.997\n",
      "local test set accuracy:  0.912\n",
      "local training set accuracy log:  0.997\n",
      "local test set accuracy log:  0.912\n",
      "group  2 peer  9\n",
      "(1000, 5) (1000, 5)\n",
      "global training set accuracy:  0.994\n",
      "global test set accuracy:  0.904\n",
      "global training set accuracy log:  0.994\n",
      "global test set accuracy log:  0.904\n",
      "group  2 peer  10\n",
      "(1000, 5) (1000, 5)\n",
      "local training set accuracy:  0.988\n",
      "local test set accuracy:  0.882\n",
      "local training set accuracy log:  0.988\n",
      "local test set accuracy log:  0.882\n",
      "group  2 peer  10\n",
      "(1000, 5) (1000, 5)\n",
      "global training set accuracy:  0.997\n",
      "global test set accuracy:  0.906\n",
      "global training set accuracy log:  0.997\n",
      "global test set accuracy log:  0.906\n"
     ]
    }
   ],
   "source": [
    "global_accuracies = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(float))))\n",
    "for group in groups:\n",
    "    for peer in peers:\n",
    "        \n",
    "        p = int(peer) - 8089\n",
    "        exec(f\"acc_train_local, acc_test_local, df_tot_l, df_tst_l = read_model_and_evaluate({group}, {p}, 'local')\")\n",
    "        global_accuracies[group][peer]['train']['local'] = acc_train_local\n",
    "        global_accuracies[group][peer]['test']['local'] = acc_test_local\n",
    "        \n",
    "        exec(f\"acc_train_global, acc_test_global, df_tot_g, df_tst_g = read_model_and_evaluate({group}, {p}, 'global')\")\n",
    "        global_accuracies[group][peer]['train']['global'] = acc_train_global\n",
    "        global_accuracies[group][peer]['test']['global'] = acc_test_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a643fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'my_defaultdict' is your existing defaultdict\n",
    "# Convert it to a regular dictionary\n",
    "regular_dict = defaultdict_to_dict(global_accuracies)\n",
    "\n",
    "# Serialize and save to a file\n",
    "with open('localnglobal_accuracies_allgroups_allpeers.pkl', 'wb') as file:\n",
    "    pickle.dump(regular_dict, file)\n",
    "\n",
    "# To load and optionally convert back to defaultdict\n",
    "# (You'll need to redefine your defaultdict structure as before)\n",
    "with open('localnglobal_accuracies_allgroups_allpeers.pkl', 'rb') as file:\n",
    "    loaded_dict = pickle.load(file)\n",
    "    # Optionally convert back to defaultdict\n",
    "    # my_defaultdict = convert_to_defaultdict(loaded_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d03e7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load and optionally convert back to defaultdict\n",
    "# (You'll need to redefine your defaultdict structure as before)\n",
    "with open('localnglobal_accuracies_allgroups_allpeers.pkl', 'rb') as file:\n",
    "    global_accuracies = pickle.load(file)\n",
    "    # Optionally convert back to defaultdict\n",
    "    # my_defaultdict = convert_to_defaultdict(loaded_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56730e20",
   "metadata": {},
   "source": [
    "### CALCULATING ACCURACIES ON TOP5 FOR EACH PEER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a5c94c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group 1 peer 1 mode global\n",
      "group 1 peer 2 mode global\n",
      "group 1 peer 3 mode global\n",
      "group 1 peer 4 mode global\n",
      "group 1 peer 5 mode global\n",
      "group 1 peer 6 mode global\n",
      "group 1 peer 7 mode global\n",
      "group 1 peer 8 mode global\n",
      "group 1 peer 9 mode global\n",
      "(16165, 5) (16168, 5)\n",
      "(16165, 5) (16168, 5)\n",
      "(16165, 5) (16168, 5)\n",
      "(16165, 5) (16168, 5)\n",
      "group 1 peer 10 mode global\n",
      "(16165, 5) (16168, 5)\n",
      "(16165, 5) (16168, 5)\n",
      "(16165, 5) (16168, 5)\n",
      "(16165, 5) (16168, 5)\n",
      "(16165, 5) (16168, 5)\n",
      "(16165, 5) (16168, 5)\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 6000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 6000 queries\n",
      "Processed 6000 queries\n",
      "Processed 5000 queries\n",
      "Processed 6000 queries\n",
      "Processed 6000 queries\n",
      "Processed 6000 queries\n",
      "Processed 6000 queries\n",
      "Processed 7000 queries\n",
      "Processed 7000 queries\n",
      "Processed 7000 queries\n",
      "Processed 6000 queries\n",
      "Processed 6000 queries\n",
      "Processed 7000 queries\n",
      "Processed 6000 queries\n",
      "Processed 7000 queries\n",
      "Processed 7000 queries\n",
      "Processed 7000 queries\n",
      "Processed 8000 queries\n",
      "Processed 8000 queries\n",
      "Processed 8000 queries\n",
      "Processed 8000 queries\n",
      "Processed 7000 queries\n",
      "Processed 7000 queries\n",
      "Processed 8000 queries\n",
      "Processed 8000 queries\n",
      "Processed 7000 queries\n",
      "Processed 8000 queries\n",
      "Processed 9000 queries\n",
      "Processed 9000 queries\n",
      "Processed 9000 queries\n",
      "Processed 9000 queries\n",
      "Processed 9000 queries\n",
      "Processed 9000 queries\n",
      "Processed 8000 queries\n",
      "Processed 8000 queries\n",
      "Processed 9000 queries\n",
      "Processed 8000 queries\n",
      "Processed 10000 queries\n",
      "Processed 10000 queries\n",
      "Processed 10000 queries\n",
      "Processed 10000 queries\n",
      "Processed 10000 queries\n",
      "Processed 10000 queries\n",
      "Processed 9000 queries\n",
      "Processed 10000 queries\n",
      "Processed 9000 queries\n",
      "Processed 11000 queries\n",
      "Processed 9000 queries\n",
      "Processed 11000 queries\n",
      "Processed 11000 queries\n",
      "Processed 11000 queries\n",
      "Processed 11000 queries\n",
      "Processed 11000 queries\n",
      "Processed 11000 queries\n",
      "Processed 12000 queries\n",
      "Processed 10000 queries\n",
      "Processed 10000 queries\n",
      "Processed 12000 queries\n",
      "Processed 12000 queries\n",
      "Processed 10000 queries\n",
      "Processed 12000 queries\n",
      "Processed 12000 queries\n",
      "Processed 12000 queries\n",
      "Processed 13000 queries\n",
      "Processed 12000 queries\n",
      "Processed 13000 queries\n",
      "Processed 13000 queries\n",
      "Processed 11000 queries\n",
      "Processed 11000 queries\n",
      "Processed 11000 queries\n",
      "Processed 13000 queries\n",
      "Processed 13000 queries\n",
      "Processed 13000 queries\n",
      "Processed 14000 queries\n",
      "Processed 14000 queries\n",
      "Processed 13000 queries\n",
      "Processed 14000 queries\n",
      "Processed 12000 queries\n",
      "Processed 12000 queries\n",
      "Processed 14000 queries\n",
      "Processed 12000 queries\n",
      "Processed 14000 queries\n",
      "Processed 14000 queries\n",
      "Processed 15000 queries\n",
      "Processed 15000 queries\n",
      "Processed 15000 queries\n",
      "Processed 14000 queries\n",
      "Processed 13000 queries\n",
      "Processed 15000 queries\n",
      "Processed 13000 queries\n",
      "Processed 15000 queries\n",
      "Processed 16000 queries\n",
      "Processed 13000 queries\n",
      "Processed 15000 queries\n",
      "Processed 16000 queries\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.930789213260762\n",
      "finished global work for group 1 and peer 5, acc test global :0.930789213260762\n",
      "Processed 16000 queries\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.9260885700148441\n",
      "finished global work for group 1 and peer 6, acc test global :0.9260885700148441\n",
      "Processed 15000 queries\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.931840672934191\n",
      "finished global work for group 1 and peer 4, acc test global :0.931840672934191\n",
      "Processed 16000 queries\n",
      "Processed 14000 queries\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.9308510638297872\n",
      "finished global work for group 1 and peer 2, acc test global :0.9308510638297872\n",
      "Processed 14000 queries\n",
      "Processed 16000 queries\n",
      "Processed 16000 queries\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.9278203859475507\n",
      "finished global work for group 1 and peer 10, acc test global :0.9278203859475507\n",
      "Processed 14000 queries\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.929490351311232\n",
      "finished global work for group 1 and peer 1, acc test global :0.929490351311232\n",
      "Processed 16000 queries\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.9306655121227115\n",
      "finished global work for group 1 and peer 7, acc test global :0.9306655121227115\n",
      "Processed 15000 queries\n",
      "Processed 15000 queries\n",
      "Processed 15000 queries\n",
      "Processed 16000 queries\n",
      "Processed 16000 queries\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.9336961900049481\n",
      "finished global work for group 1 and peer 3, acc test global :0.9336961900049481\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.9272637308263236\n",
      "finished global work for group 1 and peer 9, acc test global :0.9272637308263236\n",
      "Processed 16000 queries\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.9349332013854528\n",
      "finished global work for group 1 and peer 8, acc test global :0.9349332013854528\n",
      "group 2 peer 1 mode global\n",
      "group 2 peer 2 mode global\n",
      "group 2 peer 3 mode global\n",
      "group 2 peer 4 mode global\n",
      "group 2 peer 5 mode global\n",
      "group 2 peer 6 mode global\n",
      "group 2 peer 7 mode global\n",
      "group 2 peer 8 mode global\n",
      "group 2 peer 9 mode global\n",
      "group 2 peer 10 mode global\n",
      "(17688, 5) (17685, 5)\n",
      "(17688, 5) (17685, 5)\n",
      "(17688, 5) (17685, 5)\n",
      "(17688, 5) (17685, 5)\n",
      "(17688, 5) (17685, 5)\n",
      "(17688, 5) (17685, 5)\n",
      "(17688, 5) (17685, 5)\n",
      "(17688, 5) (17685, 5)\n",
      "(17688, 5) (17685, 5)\n",
      "(17688, 5) (17685, 5)\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 4000 queries\n",
      "Processed 3000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 4000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 6000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 6000 queries\n",
      "Processed 6000 queries\n",
      "Processed 6000 queries\n",
      "Processed 6000 queries\n",
      "Processed 6000 queries\n",
      "Processed 6000 queries\n",
      "Processed 5000 queries\n",
      "Processed 7000 queries\n",
      "Processed 6000 queries\n",
      "Processed 7000 queries\n",
      "Processed 7000 queries\n",
      "Processed 6000 queries\n",
      "Processed 7000 queries\n",
      "Processed 7000 queries\n",
      "Processed 7000 queries\n",
      "Processed 7000 queries\n",
      "Processed 8000 queries\n",
      "Processed 6000 queries\n",
      "Processed 8000 queries\n",
      "Processed 8000 queries\n",
      "Processed 7000 queries\n",
      "Processed 8000 queries\n",
      "Processed 7000 queries\n",
      "Processed 8000 queries\n",
      "Processed 8000 queries\n",
      "Processed 8000 queries\n",
      "Processed 9000 queries\n",
      "Processed 9000 queries\n",
      "Processed 9000 queries\n",
      "Processed 9000 queries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 7000 queries\n",
      "Processed 8000 queries\n",
      "Processed 9000 queries\n",
      "Processed 8000 queries\n",
      "Processed 9000 queries\n",
      "Processed 10000 queries\n",
      "Processed 9000 queries\n",
      "Processed 10000 queries\n",
      "Processed 10000 queries\n",
      "Processed 10000 queries\n",
      "Processed 9000 queries\n",
      "Processed 10000 queries\n",
      "Processed 8000 queries\n",
      "Processed 11000 queries\n",
      "Processed 10000 queries\n",
      "Processed 9000 queries\n",
      "Processed 10000 queries\n",
      "Processed 11000 queries\n",
      "Processed 11000 queries\n",
      "Processed 11000 queries\n",
      "Processed 11000 queries\n",
      "Processed 12000 queries\n",
      "Processed 10000 queries\n",
      "Processed 11000 queries\n",
      "Processed 11000 queries\n",
      "Processed 10000 queries\n",
      "Processed 9000 queries\n",
      "Processed 12000 queries\n",
      "Processed 12000 queries\n",
      "Processed 12000 queries\n",
      "Processed 13000 queries\n",
      "Processed 12000 queries\n",
      "Processed 11000 queries\n",
      "Processed 12000 queries\n",
      "Processed 12000 queries\n",
      "Processed 13000 queries\n",
      "Processed 11000 queries\n",
      "Processed 13000 queries\n",
      "Processed 10000 queries\n",
      "Processed 13000 queries\n",
      "Processed 14000 queries\n",
      "Processed 13000 queries\n",
      "Processed 13000 queries\n",
      "Processed 12000 queries\n",
      "Processed 13000 queries\n",
      "Processed 14000 queries\n",
      "Processed 14000 queries\n",
      "Processed 12000 queries\n",
      "Processed 14000 queries\n",
      "Processed 15000 queries\n",
      "Processed 11000 queries\n",
      "Processed 14000 queries\n",
      "Processed 14000 queries\n",
      "Processed 14000 queries\n",
      "Processed 15000 queries\n",
      "Processed 15000 queries\n",
      "Processed 13000 queries\n",
      "Processed 15000 queries\n",
      "Processed 13000 queries\n",
      "Processed 16000 queries\n",
      "Processed 15000 queries\n",
      "Processed 12000 queries\n",
      "Processed 15000 queries\n",
      "Processed 16000 queries\n",
      "Processed 16000 queries\n",
      "Processed 15000 queries\n",
      "Processed 14000 queries\n",
      "Processed 16000 queries\n",
      "Processed 17000 queries\n",
      "Processed 14000 queries\n",
      "Processed 16000 queries\n",
      "Processed 17000 queries\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.934520780322307\n",
      "finished global work for group 2 and peer 2, acc test global :0.934520780322307\n",
      "Processed 17000 queries\n",
      "Processed 16000 queries\n",
      "Processed 16000 queries\n",
      "Processed 13000 queries\n",
      "Processed 17000 queries\n",
      "Processed 15000 queries\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.9357647724059938\n",
      "finished global work for group 2 and peer 10, acc test global :0.9357647724059938\n",
      "Processed 15000 queries\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.9336726039016116\n",
      "finished global work for group 2 and peer 3, acc test global :0.9336726039016116\n",
      "Processed 17000 queries\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.9374611252473848\n",
      "finished global work for group 2 and peer 7, acc test global :0.9374611252473848\n",
      "Processed 17000 queries\n",
      "Processed 17000 queries\n",
      "Processed 14000 queries\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.9399491094147583\n",
      "finished global work for group 2 and peer 9, acc test global :0.9399491094147583\n",
      "Processed 16000 queries\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.9346338705117331\n",
      "finished global work for group 2 and peer 8, acc test global :0.9346338705117331\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.9352558665535765\n",
      "finished global work for group 2 and peer 5, acc test global :0.9352558665535765\n",
      "Processed 16000 queries\n",
      "Processed 17000 queries\n",
      "Processed 15000 queries\n",
      "Processed 17000 queries\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.936104042974272\n",
      "finished global work for group 2 and peer 6, acc test global :0.936104042974272\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.936104042974272\n",
      "finished global work for group 2 and peer 4, acc test global :0.936104042974272\n",
      "Processed 16000 queries\n",
      "Processed 17000 queries\n",
      "global training set accuracy:  -1.0\n",
      "global test set accuracy:  0.9341815097540288\n",
      "finished global work for group 2 and peer 1, acc test global :0.9341815097540288\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "\n",
    "# Global dictionary to store models for each group and peer\n",
    "global_objects = {}\n",
    "global_accuracies = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(float))))\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, group, peer):\n",
    "        self.group = group\n",
    "        self.peer = peer\n",
    "        self.counter = 0\n",
    "\n",
    "    def read_model_and_evaluate(self, mode='global'):\n",
    "        global global_accuracies\n",
    "        acc_train = -1.0\n",
    "        acc_test = -1.0\n",
    "        print('group', self.group, 'peer', self.peer, 'mode', mode)\n",
    "\n",
    "        model = globals()[f'model_group{self.group}_peer{self.peer}']\n",
    "        \n",
    "        if mode == 'global':\n",
    "            df_tot = globals()[f'train_df_group{group}'].copy()\n",
    "            self.df_tst = globals()[f'test_df_group{group}'].copy()\n",
    "        elif mode == 'local':\n",
    "            df_tot = globals()[f'train_df_group{group}_peer{peer}'].copy()\n",
    "            self.df_tst = globals()[f'test_df_group{group}_peer{peer}'].copy()\n",
    "        \n",
    "        print(df_tot.shape, self.df_tst.shape)\n",
    "\n",
    "        self.df_tst['generated_doc_id'] = self.df_tst['query'].apply(lambda x: self.generate_text_beams(x, model))\n",
    "        acc_test = self.df_tst.apply(lambda row: row['doc_id'] in row['generated_doc_id'], axis=1).sum() / self.df_tst.shape[0]\n",
    "        \n",
    "        \n",
    "        global global_objects\n",
    "        global_objects[(self.group, self.peer)] = self.df_tst\n",
    "        \n",
    "        print(f'{mode} training set accuracy: ', acc_train)\n",
    "        print(f'{mode} test set accuracy: ', acc_test)\n",
    "        return acc_train, acc_test\n",
    "\n",
    "    def generate_text_beams(self, query, model):\n",
    "        self.counter += 1\n",
    "        if self.counter % 1000 == 0:\n",
    "            print(f\"Processed {self.counter} queries\")\n",
    "        input_ids = tokenizer.encode(query, return_tensors='pt')\n",
    "        output = model.generate(input_ids, do_sample=False, max_length=20,\n",
    "                                num_beams=5, num_return_sequences=5)\n",
    "        return [tokenizer.decode(i, skip_special_tokens=True) for i in output]\n",
    "\n",
    "    def thread_function(self):\n",
    "        global global_accuracies_20samples\n",
    "        acc_train_global, acc_test_global = self.read_model_and_evaluate('global')\n",
    "        global_accuracies[self.group][self.peer]['train']['global'] = acc_train_global\n",
    "        global_accuracies[self.group][self.peer]['test']['global'] = acc_test_global\n",
    "        print(f'finished global work for group {self.group} and peer {self.peer}, acc test global :{acc_test_global}')\n",
    "\n",
    "def evaluate_in_thread(group, peer):\n",
    "    try:\n",
    "        evaluator = ModelEvaluator(group, peer)\n",
    "        evaluator.thread_function()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in thread for group {group} and peer {peer}: {e}\")\n",
    "\n",
    "    \n",
    "# Start threads directly in the main script body\n",
    "threads = []\n",
    "for group in groups:\n",
    "    for peer in peers:\n",
    "        p = int(peer) - 8089\n",
    "        thread = threading.Thread(target=evaluate_in_thread, args=(group, p,))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "\n",
    "    # Wait for all threads to complete\n",
    "    for thread in threads:\n",
    "        thread.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "941f5575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'1': defaultdict(<function __main__.<lambda>.<locals>.<lambda>()>,\n",
       "                         {5: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.930789213260762})}),\n",
       "                          6: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.9260885700148441})}),\n",
       "                          4: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.931840672934191})}),\n",
       "                          2: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.9308510638297872})}),\n",
       "                          10: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.9278203859475507})}),\n",
       "                          1: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.929490351311232})}),\n",
       "                          7: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.9306655121227115})}),\n",
       "                          3: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.9336961900049481})}),\n",
       "                          9: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.9272637308263236})}),\n",
       "                          8: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.9349332013854528})})}),\n",
       "             '2': defaultdict(<function __main__.<lambda>.<locals>.<lambda>()>,\n",
       "                         {2: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.934520780322307})}),\n",
       "                          10: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.9357647724059938})}),\n",
       "                          3: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.9336726039016116})}),\n",
       "                          7: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.9374611252473848})}),\n",
       "                          9: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.9399491094147583})}),\n",
       "                          8: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.9346338705117331})}),\n",
       "                          5: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.9352558665535765})}),\n",
       "                          6: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.936104042974272})}),\n",
       "                          4: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.936104042974272})}),\n",
       "                          1: defaultdict(<function __main__.<lambda>.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                                      {'train': defaultdict(float,\n",
       "                                                   {'global': -1.0}),\n",
       "                                       'test': defaultdict(float,\n",
       "                                                   {'global': 0.9341815097540288})})})})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b620405c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'my_defaultdict' is your existing defaultdict\n",
    "# Convert it to a regular dictionary\n",
    "regular_dict = defaultdict_to_dict(global_accuracies)\n",
    "\n",
    "# Serialize and save to a file\n",
    "with open('global_accuracies_5beams.pkl', 'wb') as file:\n",
    "    pickle.dump(regular_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2bcdeb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': {5: {'train': {'global': -1.0}, 'test': {'global': 0.930789213260762}},\n",
       "  6: {'train': {'global': -1.0}, 'test': {'global': 0.9260885700148441}},\n",
       "  4: {'train': {'global': -1.0}, 'test': {'global': 0.931840672934191}},\n",
       "  2: {'train': {'global': -1.0}, 'test': {'global': 0.9308510638297872}},\n",
       "  10: {'train': {'global': -1.0}, 'test': {'global': 0.9278203859475507}},\n",
       "  1: {'train': {'global': -1.0}, 'test': {'global': 0.929490351311232}},\n",
       "  7: {'train': {'global': -1.0}, 'test': {'global': 0.9306655121227115}},\n",
       "  3: {'train': {'global': -1.0}, 'test': {'global': 0.9336961900049481}},\n",
       "  9: {'train': {'global': -1.0}, 'test': {'global': 0.9272637308263236}},\n",
       "  8: {'train': {'global': -1.0}, 'test': {'global': 0.9349332013854528}}},\n",
       " '2': {2: {'train': {'global': -1.0}, 'test': {'global': 0.934520780322307}},\n",
       "  10: {'train': {'global': -1.0}, 'test': {'global': 0.9357647724059938}},\n",
       "  3: {'train': {'global': -1.0}, 'test': {'global': 0.9336726039016116}},\n",
       "  7: {'train': {'global': -1.0}, 'test': {'global': 0.9374611252473848}},\n",
       "  9: {'train': {'global': -1.0}, 'test': {'global': 0.9399491094147583}},\n",
       "  8: {'train': {'global': -1.0}, 'test': {'global': 0.9346338705117331}},\n",
       "  5: {'train': {'global': -1.0}, 'test': {'global': 0.9352558665535765}},\n",
       "  6: {'train': {'global': -1.0}, 'test': {'global': 0.936104042974272}},\n",
       "  4: {'train': {'global': -1.0}, 'test': {'global': 0.936104042974272}},\n",
       "  1: {'train': {'global': -1.0}, 'test': {'global': 0.9341815097540288}}}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('global_accuracies_5beams.pkl', 'rb') as file:\n",
    "    loaded_dict = pickle.load(file)\n",
    "    # Optionally convert back to defaultdict\n",
    "    # my_defaultdict = convert_to_defaultdict(loaded_dict)\n",
    "    \n",
    "loaded_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98d88a2",
   "metadata": {},
   "source": [
    "# Sampling random models and aggregating their suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13f25364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group 1\n",
      "peer 1\n",
      "peer 2\n",
      "peer 3\n",
      "peer 4\n",
      "peer 5\n",
      "peer 6\n",
      "peer 7\n",
      "peer 8\n",
      "peer 9\n",
      "peer 10\n",
      "group 2\n",
      "peer 1\n",
      "peer 2\n",
      "peer 3\n",
      "peer 4\n",
      "peer 5\n",
      "peer 6\n",
      "peer 7\n",
      "peer 8\n",
      "peer 9\n",
      "peer 10\n"
     ]
    }
   ],
   "source": [
    "three_groups_list = defaultdict(list)\n",
    "for group in groups:\n",
    "    print (f'group {group}')\n",
    "    for i, peer in enumerate(peers):\n",
    "        print (f'peer {int(peer) - 8089}')\n",
    "        exec(f'three_groups_list[\"group{group}\"].append(model_group{group}_peer{int(peer)-8089})')\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5eaaf6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set size:test set size: 17685\n",
      " 16168\n",
      "Processed 500 queries\n",
      "Processed 500 queries\n",
      "Processed 1000 queries\n",
      "Processed 1000 queries\n",
      "Processed 1500 queries\n",
      "Processed 1500 queries\n",
      "Processed 2000 queries\n",
      "Processed 2000 queries\n",
      "Processed 2500 queries\n",
      "Processed 2500 queries\n",
      "Processed 3000 queries\n",
      "Processed 3000 queries\n",
      "Processed 3500 queries\n",
      "Processed 3500 queries\n",
      "Processed 4000 queries\n",
      "Processed 4000 queries\n",
      "Processed 4500 queries\n",
      "Processed 4500 queries\n",
      "Processed 5000 queries\n",
      "Processed 5000 queries\n",
      "Processed 5500 queries\n",
      "Processed 5500 queries\n",
      "Processed 6000 queries\n",
      "Processed 6000 queries\n",
      "Processed 6500 queries\n",
      "Processed 6500 queries\n",
      "Processed 7000 queries\n",
      "Processed 7000 queries\n",
      "Processed 7500 queries\n",
      "Processed 7500 queries\n",
      "Processed 8000 queries\n",
      "Processed 8000 queries\n",
      "Processed 8500 queries\n",
      "Processed 8500 queries\n",
      "Processed 9000 queries\n",
      "Processed 9000 queries\n",
      "Processed 9500 queries\n",
      "Processed 9500 queries\n",
      "Processed 10000 queries\n",
      "Processed 10000 queries\n",
      "Processed 10500 queries\n",
      "Processed 10500 queries\n",
      "Processed 11000 queries\n",
      "Processed 11000 queries\n",
      "Processed 11500 queries\n",
      "Processed 11500 queries\n",
      "Processed 12000 queries\n",
      "Processed 12000 queries\n",
      "Processed 12500 queries\n",
      "Processed 12500 queries\n",
      "Processed 13000 queries\n",
      "Processed 13000 queries\n",
      "Processed 13500 queries\n",
      "Processed 13500 queries\n",
      "Processed 14000 queries\n",
      "Processed 14000 queries\n",
      "Processed 14500 queries\n",
      "Processed 14500 queries\n",
      "Processed 15000 queries\n",
      "Processed 15000 queries\n",
      "Processed 15500 queries\n",
      "Processed 15500 queries\n",
      "Processed 16000 queries\n",
      "Processed 16000 queries\n",
      "Group: 1, Test Acc: 0.9447674418604651\n",
      "Processed 16500 queries\n",
      "Processed 17000 queries\n",
      "Processed 17500 queries\n",
      "Group: 2, Test Acc: 0.9494486853265479\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "import threading\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "class ModelManager:\n",
    "    def __init__(self, model_list, test_df, tokenizer):\n",
    "        self.model_list = model_list\n",
    "        self.test_df = test_df.copy()\n",
    "        print ('test set size:', self.test_df.shape[0])\n",
    "    \n",
    "        self.tokenizer = tokenizer\n",
    "        self.counter = 0\n",
    "\n",
    "    def generate_text_beams(self, query):\n",
    "        self.counter += 1\n",
    "        if self.counter % 500 == 0:\n",
    "            print(f\"Processed {self.counter} queries\")\n",
    "        results = defaultdict(float)\n",
    "        \n",
    "        sampled_models = random.sample(self.model_list['group1'], 3)\n",
    "        sampled_models.extend(random.sample(self.model_list['group2'], 3))\n",
    "#         sampled_models.extend(random.sample(self.model_list['group3'], 3))\n",
    "        for model in sampled_models:\n",
    "            # Ensure query is properly encoded\n",
    "            input_ids = self.tokenizer.encode(query, return_tensors='pt')\n",
    "            output = model.generate(input_ids, do_sample=False, return_dict_in_generate=True, output_scores=True,\n",
    "                                    num_beams=5, num_return_sequences=5, max_length = 20)\n",
    "\n",
    "            beam_scores = output.sequences_scores\n",
    "#             print (beam_scores)\n",
    "            probabilities = softmax(beam_scores, dim=0).tolist()\n",
    "#             print (probabilities)\n",
    "            model_res = [self.tokenizer.decode(output_id, skip_special_tokens=True) for output_id in output.sequences]\n",
    "            \n",
    "\n",
    "            for res, prob in zip(model_res, probabilities):\n",
    "                results[res] += prob\n",
    "\n",
    "        return self.top_5_generated_texts(results)\n",
    "\n",
    "    def top_5_generated_texts(self, input_dict):\n",
    "        sorted_responses = sorted(input_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [response[0] for response in sorted_responses[:5]]\n",
    "\n",
    "    def evaluate_accuracy(self):\n",
    "#         self.train_df['generated_doc_id'] = self.train_df['query'].apply(self.generate_text_beams)\n",
    "        self.test_df['generated_doc_id'] = self.test_df['query'].apply(self.generate_text_beams)\n",
    "\n",
    "#         acc_train = self.calculate_accuracy(self.train_df)\n",
    "        acc_test = self.calculate_accuracy(self.test_df)\n",
    "\n",
    "        return acc_test\n",
    "\n",
    "    def calculate_accuracy(self, df):\n",
    "        return df.apply(lambda row: row['doc_id'] in row['generated_doc_id'], axis=1).sum() / df.shape[0]\n",
    "\n",
    "# Assuming you have predefined dictionaries/lists for models and datasets, such as:\n",
    "# two_groups_list = [...]\n",
    "# three_groups_list = [...]\n",
    "# train_df_group1 = ...\n",
    "# test_df_group1 = ...\n",
    "# ... and so on for other groups\n",
    "# And a tokenizer instance\n",
    "\n",
    "def run_evaluation(group_nbr, tokenizer):\n",
    "    model_list = three_groups_list\n",
    "\n",
    "    test_df = globals()[f'test_df_group{group_nbr}']\n",
    "    \n",
    "    manager = ModelManager(model_list, test_df, tokenizer)\n",
    "    \n",
    "    key = group_nbr\n",
    "    model_managers[key] = manager\n",
    "    \n",
    "    \n",
    "    acc_test = manager.evaluate_accuracy()\n",
    "    global_accuracies[key] = {'acc_test': acc_test}\n",
    "\n",
    "    print(f\"Group: {group_nbr}, Test Acc: {acc_test}\")\n",
    "\n",
    "    \n",
    "# Global collection to store ModelManager instances\n",
    "model_managers = {}\n",
    "global_accuracies = {}\n",
    "    \n",
    "# Threading\n",
    "threads = []\n",
    "\n",
    "for group_nbr in groups:\n",
    "        thread = threading.Thread(target=run_evaluation, args=(group_nbr, tokenizer))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9be52d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.8974271981905569\n",
      "1 0.8875556655121227\n"
     ]
    }
   ],
   "source": [
    "for group in model_managers:\n",
    "    df_temp = model_managers[group].test_df.copy()\n",
    "    df_temp['generated_doc_id_1beam'] = df_temp['generated_doc_id'].apply(lambda x: [x[0]])\n",
    "    print (group, df_temp.apply(lambda row: row['doc_id'] in row['generated_doc_id_1beam'], \n",
    "                                axis=1).sum() / df_temp.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70580685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'my_defaultdict' is your existing defaultdict\n",
    "# Convert it to a regular dictionary\n",
    "regular_dict = defaultdict_to_dict(global_accuracies)\n",
    "\n",
    "# Serialize and save to a file\n",
    "with open('inter_group_accs_5beams.pkl', 'wb') as file:\n",
    "    pickle.dump(regular_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e36e2d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': {'acc_test': 0.9447674418604651}, '2': {'acc_test': 0.9494486853265479}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('inter_group_accs_5beams.pkl', 'rb') as file:\n",
    "    loaded_dict = pickle.load(file)\n",
    "    # Optionally convert back to defaultdict\n",
    "    # my_defaultdict = convert_to_defaultdict(loaded_dict)\n",
    "    \n",
    "loaded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f670d78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.8974271981905569\n",
      "1 0.8875556655121227\n"
     ]
    }
   ],
   "source": [
    "regular_dict2 = {}\n",
    "for group in model_managers:\n",
    "    df_temp = model_managers[group].test_df.copy()\n",
    "    df_temp['generated_doc_id_1beam'] = df_temp['generated_doc_id'].apply(lambda x: [x[0]])\n",
    "    regular_dict2[group] = df_temp.apply(lambda row: row['doc_id'] in row['generated_doc_id_1beam'], \n",
    "                                axis=1).sum() / df_temp.shape[0]\n",
    "    print (group, df_temp.apply(lambda row: row['doc_id'] in row['generated_doc_id_1beam'], \n",
    "                                axis=1).sum() / df_temp.shape[0])\n",
    "    \n",
    "    \n",
    "# display(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "300a88ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('inter_group_accs_1beams.pkl', 'wb') as file:\n",
    "    pickle.dump(regular_dict2, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3f561b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2': 0.8974271981905569, '1': 0.8875556655121227}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('inter_group_accs_1beams.pkl', 'rb') as file:\n",
    "    loaded_dict = pickle.load(file)\n",
    "    # Optionally convert back to defaultdict\n",
    "    # my_defaultdict = convert_to_defaultdict(loaded_dict)\n",
    "    \n",
    "loaded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5ba140",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
