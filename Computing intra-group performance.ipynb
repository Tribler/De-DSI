{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7262bf67",
   "metadata": {},
   "source": [
    "# This is the notebook which tests the performance of the trained models, after training was performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35fb0141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got here 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AdamW\n",
    "import pickle\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "# Reading data and models\n",
    "groups = [str(i) for i in range(1,2)]\n",
    "peers = [str(i) for i in range(8090, 8100)]\n",
    "number_of_models_sampled = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "786849ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_first_files_with_str(directory, str_contain, index):\n",
    "    \"\"\"\n",
    "    Returns the model given \n",
    "    - the directory, \n",
    "    - the model's specific name \n",
    "    - the number representing the version of the trained model\n",
    "    \n",
    "    It may happen that some models have trained more than others due to \n",
    "    a quirk of how the parallelization is performed. In this case,\n",
    "    I recommend choosing the lowest index possible for which all models were trained.\n",
    "    \"\"\"\n",
    "    # Create an empty list to store files that contain str_contain\n",
    "    files_with_str = []\n",
    "    print (f'searching for {directory}  + {str_contain} + {index}')\n",
    "    # Iterate over the files in the directory\n",
    "    for file in os.listdir(directory):\n",
    "        if file[:4] == str_contain:\n",
    "            files_with_str.append(file)\n",
    "    \n",
    "    print (files_with_str)\n",
    "    # Sort only the files that contain 'x1'\n",
    "    files_with_str.sort()\n",
    "    print (files_with_str)\n",
    "\n",
    "    # Return the first file in the sorted list, or None if the list is empty\n",
    "    return files_with_str[index] if files_with_str else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0865bbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "searching for aggregated_results/group1/models  + 8090 + 2\n",
      "['8090_2024-04-11 045514_my_t5_model', '8090_2024-04-10 232229_my_t5_model', '8090_2024-04-10 212949_my_t5_model', '8090_2024-04-11 030343_my_t5_model', '8090_2024-04-10 193648_my_t5_model', '8090_2024-04-11 011310_my_t5_model']\n",
      "['8090_2024-04-10 193648_my_t5_model', '8090_2024-04-10 212949_my_t5_model', '8090_2024-04-10 232229_my_t5_model', '8090_2024-04-11 011310_my_t5_model', '8090_2024-04-11 030343_my_t5_model', '8090_2024-04-11 045514_my_t5_model']\n",
      "1 8090 8090_2024-04-10 232229_my_t5_model\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 33\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m (group, peer, model_file)\n\u001b[1;32m     30\u001b[0m exec_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_group\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgroup\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_peer\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mint\u001b[39m(peer)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m8089\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m = T5ForConditionalGeneration.from_pretrained(os.path.join(model_folder, model_file))\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 33\u001b[0m exec(exec_str)\n",
      "File \u001b[0;32m<string>:1\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:3480\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3471\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3472\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3473\u001b[0m     (\n\u001b[1;32m   3474\u001b[0m         model,\n\u001b[1;32m   3475\u001b[0m         missing_keys,\n\u001b[1;32m   3476\u001b[0m         unexpected_keys,\n\u001b[1;32m   3477\u001b[0m         mismatched_keys,\n\u001b[1;32m   3478\u001b[0m         offload_index,\n\u001b[1;32m   3479\u001b[0m         error_msgs,\n\u001b[0;32m-> 3480\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_load_pretrained_model(\n\u001b[1;32m   3481\u001b[0m         model,\n\u001b[1;32m   3482\u001b[0m         state_dict,\n\u001b[1;32m   3483\u001b[0m         loaded_state_dict_keys,  \u001b[38;5;66;03m# XXX: rename?\u001b[39;00m\n\u001b[1;32m   3484\u001b[0m         resolved_archive_file,\n\u001b[1;32m   3485\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   3486\u001b[0m         ignore_mismatched_sizes\u001b[38;5;241m=\u001b[39mignore_mismatched_sizes,\n\u001b[1;32m   3487\u001b[0m         sharded_metadata\u001b[38;5;241m=\u001b[39msharded_metadata,\n\u001b[1;32m   3488\u001b[0m         _fast_init\u001b[38;5;241m=\u001b[39m_fast_init,\n\u001b[1;32m   3489\u001b[0m         low_cpu_mem_usage\u001b[38;5;241m=\u001b[39mlow_cpu_mem_usage,\n\u001b[1;32m   3490\u001b[0m         device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[1;32m   3491\u001b[0m         offload_folder\u001b[38;5;241m=\u001b[39moffload_folder,\n\u001b[1;32m   3492\u001b[0m         offload_state_dict\u001b[38;5;241m=\u001b[39moffload_state_dict,\n\u001b[1;32m   3493\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mtorch_dtype,\n\u001b[1;32m   3494\u001b[0m         is_quantized\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mBITS_AND_BYTES),\n\u001b[1;32m   3495\u001b[0m         keep_in_fp32_modules\u001b[38;5;241m=\u001b[39mkeep_in_fp32_modules,\n\u001b[1;32m   3496\u001b[0m     )\n\u001b[1;32m   3498\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_4bit \u001b[38;5;241m=\u001b[39m load_in_4bit\n\u001b[1;32m   3499\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_8bit \u001b[38;5;241m=\u001b[39m load_in_8bit\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:3824\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m# Whole checkpoint\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     mismatched_keys \u001b[38;5;241m=\u001b[39m _find_mismatched_keys(\n\u001b[1;32m   3817\u001b[0m         state_dict,\n\u001b[1;32m   3818\u001b[0m         model_state_dict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3822\u001b[0m         ignore_mismatched_sizes,\n\u001b[1;32m   3823\u001b[0m     )\n\u001b[0;32m-> 3824\u001b[0m     error_msgs \u001b[38;5;241m=\u001b[39m _load_state_dict_into_model(model_to_load, state_dict, start_prefix)\n\u001b[1;32m   3825\u001b[0m     offload_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3826\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3827\u001b[0m     \u001b[38;5;66;03m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n\u001b[1;32m   3828\u001b[0m \n\u001b[1;32m   3829\u001b[0m     \u001b[38;5;66;03m# This should always be a list but, just to be sure.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:571\u001b[0m, in \u001b[0;36m_load_state_dict_into_model\u001b[0;34m(model_to_load, state_dict, start_prefix)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m             load(child, state_dict, prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 571\u001b[0m load(model_to_load, state_dict, prefix\u001b[38;5;241m=\u001b[39mstart_prefix)\n\u001b[1;32m    572\u001b[0m \u001b[38;5;66;03m# Delete `state_dict` so it could be collected by GC earlier. Note that `state_dict` is a copy of the argument, so\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;66;03m# it's safe to delete it.\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m state_dict\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:569\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 569\u001b[0m         load(child, state_dict, prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:569\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 569\u001b[0m         load(child, state_dict, prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "    \u001b[0;31m[... skipping similar frames: _load_state_dict_into_model.<locals>.load at line 569 (4 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:569\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 569\u001b[0m         load(child, state_dict, prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:565\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    563\u001b[0m                     module\u001b[38;5;241m.\u001b[39m_load_from_state_dict(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 565\u001b[0m         module\u001b[38;5;241m.\u001b[39m_load_from_state_dict(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:2040\u001b[0m, in \u001b[0;36mModule._load_from_state_dict\u001b[0;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[0m\n\u001b[1;32m   2038\u001b[0m                 \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, input_param)\n\u001b[1;32m   2039\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2040\u001b[0m             param\u001b[38;5;241m.\u001b[39mcopy_(input_param)\n\u001b[1;32m   2041\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m   2042\u001b[0m     error_msgs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhile copying the parameter named \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2043\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhose dimensions in the model are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2044\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhose dimensions in the checkpoint are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_param\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2045\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124man exception occurred : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mex\u001b[38;5;241m.\u001b[39margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2046\u001b[0m                       )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# reading individual peer datasets & group datasets, e.g. train_df_group1, train_df_group1_peer1 etc.\n",
    "for group in groups:\n",
    "    # creating train_df's\n",
    "    exec(f'train_df_group{group} = pd.DataFrame()')\n",
    "    for peer in peers:\n",
    "        datasets_folder = os.path.join('aggregated_results',f'group{group}','datasets')\n",
    "        exec_str = f\"train_df_group{group}_peer{int(peer) - 8089} = pd.read_csv(os.path.join(datasets_folder,'{peer}_df.csv'))\"\n",
    "        exec(exec_str)\n",
    "        exec(f'train_df_group{group} = pd.concat([train_df_group{group}, train_df_group{group}_peer{int(peer) - 8089}])')\n",
    "       \n",
    "    exec(f'train_df_group{group} = train_df_group{group}.drop_duplicates()')\n",
    "    \n",
    "    # creating test_df's: test_df_group1, test_df_group1_peer1\n",
    "    \n",
    "    datasets_folder = os.path.join('aggregated_results',f'group{group}','datasets')\n",
    "    exec (f\"test_df_group{group} = pd.read_csv(os.path.join(datasets_folder,'test_df.csv')) \")\n",
    "    exec (f\"test_df_group{group} = test_df_group{group}[test_df_group{group}['doc_id'].isin(train_df_group{group}['doc_id'].unique())]\")\n",
    "    for peer in peers:\n",
    "        exec (f\"test_df_group{group}_peer{int(peer) - 8089} = test_df_group{group}[test_df_group{group}['doc_id'].isin(train_df_group{group}_peer{int(peer) - 8089}['doc_id'].unique())]\")\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#reading models, e.g. model_group1_peer1\n",
    "for group in groups:\n",
    "    for peer in peers:\n",
    "        model_folder = os.path.join('aggregated_results',f'group{group}', 'models')\n",
    "        model_file = find_first_files_with_str(model_folder, peer, 2) # 10 is the largest number of saved models that all peers have finished training\n",
    "        print (group, peer, model_file)\n",
    "        exec_str = f\"model_group{group}_peer{str(int(peer)-8089)} = T5ForConditionalGeneration.from_pretrained(os.path.join(model_folder, model_file))\"\n",
    "        \n",
    "        \n",
    "        exec(exec_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98d88a2",
   "metadata": {},
   "source": [
    "# Sampling random models and aggregating their suggestions - 5 beams, with probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13f25364",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_group1_list = []\n",
    "models_group2_list = []\n",
    "models_group3_list = []\n",
    "\n",
    "for group in groups:\n",
    "    for i, peer in enumerate(peers):\n",
    "        exec(f'models_group{group}_list.append(model_group{group}_peer{int(peer)-8089})')\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5eaaf6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: 16165\n",
      "test set size: 16168\n",
      "Processed 300 queries\n",
      "Processed 600 queries\n",
      "Processed 900 queries\n",
      "Processed 1200 queries\n",
      "Processed 1500 queries\n",
      "Processed 1800 queries\n",
      "Processed 2100 queries\n",
      "Processed 2400 queries\n",
      "Processed 2700 queries\n",
      "Processed 3000 queries\n",
      "Processed 3300 queries\n",
      "Processed 3600 queries\n",
      "Processed 3900 queries\n",
      "Processed 4200 queries\n",
      "Processed 4500 queries\n",
      "Processed 4800 queries\n",
      "Processed 5100 queries\n",
      "Processed 5400 queries\n",
      "Processed 5700 queries\n",
      "Processed 6000 queries\n",
      "Processed 6300 queries\n",
      "Processed 6600 queries\n",
      "Processed 6900 queries\n",
      "Processed 7200 queries\n",
      "Processed 7500 queries\n",
      "Processed 7800 queries\n",
      "Processed 8100 queries\n",
      "Processed 8400 queries\n",
      "Processed 8700 queries\n",
      "Processed 9000 queries\n",
      "Processed 9300 queries\n",
      "Processed 9600 queries\n",
      "Processed 9900 queries\n",
      "Processed 10200 queries\n",
      "Processed 10500 queries\n",
      "Processed 10800 queries\n",
      "Processed 11100 queries\n",
      "Processed 11400 queries\n",
      "Processed 11700 queries\n",
      "Processed 12000 queries\n",
      "Processed 12300 queries\n",
      "Processed 12600 queries\n",
      "Processed 12900 queries\n",
      "Processed 13200 queries\n",
      "Processed 13500 queries\n",
      "Processed 13800 queries\n",
      "Processed 14100 queries\n",
      "Processed 14400 queries\n",
      "Processed 14700 queries\n",
      "Processed 15000 queries\n",
      "Processed 15300 queries\n",
      "Processed 15600 queries\n",
      "Processed 15900 queries\n",
      "Processed 16200 queries\n",
      "Processed 16500 queries\n",
      "Processed 16800 queries\n",
      "Processed 17100 queries\n",
      "Processed 17400 queries\n",
      "Processed 17700 queries\n",
      "Processed 18000 queries\n",
      "Processed 18300 queries\n",
      "Processed 18600 queries\n",
      "Processed 18900 queries\n",
      "Processed 19200 queries\n",
      "Processed 19500 queries\n",
      "Processed 19800 queries\n",
      "Processed 20100 queries\n",
      "Processed 20400 queries\n",
      "Processed 20700 queries\n",
      "Processed 21000 queries\n",
      "Processed 21300 queries\n",
      "Processed 21600 queries\n",
      "Processed 21900 queries\n",
      "Processed 22200 queries\n",
      "Processed 22500 queries\n",
      "Processed 22800 queries\n",
      "Processed 23100 queries\n",
      "Processed 23400 queries\n",
      "Processed 23700 queries\n",
      "Processed 24000 queries\n",
      "Processed 24300 queries\n",
      "Processed 24600 queries\n",
      "Processed 24900 queries\n",
      "Processed 25200 queries\n",
      "Processed 25500 queries\n",
      "Processed 25800 queries\n",
      "Processed 26100 queries\n",
      "Processed 26400 queries\n",
      "Processed 26700 queries\n",
      "Processed 27000 queries\n",
      "Processed 27300 queries\n",
      "Processed 27600 queries\n",
      "Processed 27900 queries\n",
      "Processed 28200 queries\n",
      "Processed 28500 queries\n",
      "Processed 28800 queries\n",
      "Processed 29100 queries\n",
      "Processed 29400 queries\n",
      "Processed 29700 queries\n",
      "Processed 30000 queries\n",
      "Processed 30300 queries\n",
      "Processed 30600 queries\n",
      "Processed 30900 queries\n",
      "Processed 31200 queries\n",
      "Processed 31500 queries\n",
      "Processed 31800 queries\n",
      "Processed 32100 queries\n",
      "Group: 1, Train Acc: 0.9996906897618311, Test Acc: 0.9458807521029193\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "import threading\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "class ModelManager:\n",
    "    \"\"\"Class to hold the models, tokenizer and dataframes - including with results after their calculation\"\"\"\n",
    "    def __init__(self, model_list, train_df, test_df, tokenizer):\n",
    "        self.model_list = model_list\n",
    "        self.train_df = train_df.copy()\n",
    "        self.test_df = test_df.copy()\n",
    "        \n",
    "        print ('train set size:', self.train_df.shape[0])\n",
    "        print ('test set size:', self.test_df.shape[0])\n",
    "    \n",
    "        self.tokenizer = tokenizer\n",
    "        self.counter = 0\n",
    "\n",
    "    def generate_text_beams(self, query):\n",
    "        '''Generates the result for each query in the test set'''\n",
    "        self.counter += 1\n",
    "        if self.counter % 300 == 0:\n",
    "            print(f\"Processed {self.counter} queries\")\n",
    "        results = defaultdict(float)\n",
    "        \n",
    "        # sampling 3 models from the ones available\n",
    "        sampled_models = random.sample(self.model_list, number_of_models_sampled)\n",
    "\n",
    "        # Computing the ensemble \n",
    "        for model in sampled_models:\n",
    "            input_ids = self.tokenizer.encode(query, return_tensors='pt')\n",
    "            output = model.generate(input_ids, do_sample=False, return_dict_in_generate=True, output_scores=True,\n",
    "                                    num_beams=5, num_return_sequences=5, max_length = 20)\n",
    "\n",
    "            beam_scores = output.sequences_scores\n",
    "            probabilities = softmax(beam_scores, dim=0).tolist()\n",
    "            model_res = [self.tokenizer.decode(output_id, skip_special_tokens=True) for output_id in output.sequences]\n",
    "\n",
    "        \n",
    "            for res, prob in zip(model_res, probabilities):\n",
    "                results[res] += prob\n",
    "\n",
    "        \n",
    "        return self.top_5_generated_texts(results)\n",
    "\n",
    "    def top_5_generated_texts(self, input_dict):\n",
    "        '''Returns the 5 docids that the model is most confident about'''\n",
    "        sorted_responses = sorted(input_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [response[0] for response in sorted_responses[:5]]\n",
    "\n",
    "    def evaluate_accuracy(self):\n",
    "        self.train_df['generated_doc_id'] = self.train_df['query'].apply(self.generate_text_beams)\n",
    "        self.test_df['generated_doc_id'] = self.test_df['query'].apply(self.generate_text_beams)\n",
    "\n",
    "        acc_train = self.calculate_accuracy(self.train_df)\n",
    "        acc_test = self.calculate_accuracy(self.test_df)\n",
    "\n",
    "        return acc_train, acc_test\n",
    "\n",
    "    def calculate_accuracy(self, df):\n",
    "        return df.apply(lambda row: row['doc_id'] in row['generated_doc_id'], axis=1).sum() / df.shape[0]\n",
    "\n",
    "# Assuming you have predefined dictionaries/lists for models and datasets, such as:\n",
    "# two_groups_list = [...]\n",
    "# three_groups_list = [...]\n",
    "# train_df_group1 = ...\n",
    "# test_df_group1 = ...\n",
    "# ... and so on for other groups\n",
    "# And a tokenizer instance\n",
    "\n",
    "def run_evaluation(group_nbr, tokenizer):\n",
    "    \n",
    "    \n",
    "    model_list = globals()[f'models_group{group_nbr}_list']\n",
    "    train_df = globals()[f'train_df_group{group_nbr}']\n",
    "    test_df = globals()[f'test_df_group{group_nbr}']\n",
    "    \n",
    "    manager = ModelManager(model_list, train_df, test_df, tokenizer)\n",
    "    \n",
    "    key = group_nbr\n",
    "    model_managers[key] = manager\n",
    "    \n",
    "    \n",
    "    acc_train, acc_test = manager.evaluate_accuracy()\n",
    "    global_accuracies[key] = {'acc_train': acc_train, 'acc_test': acc_test}\n",
    "\n",
    "    print(f\"Group: {group_nbr}, Train Acc: {acc_train}, Test Acc: {acc_test}\")\n",
    "\n",
    "    \n",
    "# Global collection to store ModelManager instances\n",
    "model_managers = {}\n",
    "global_accuracies = {}\n",
    "    \n",
    "# Threading\n",
    "threads = []\n",
    "\n",
    "for group_nbr in groups:\n",
    "    thread = threading.Thread(target=run_evaluation, args=(group_nbr, tokenizer))\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "329ce064",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def defaultdict_to_dict(d):\n",
    "    \"\"\" Recursively convert defaultdict to dict. \"\"\"\n",
    "    if isinstance(d, defaultdict):\n",
    "        d = {key: defaultdict_to_dict(value) for key, value in d.items()}\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70580685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'my_defaultdict' is your existing defaultdict\n",
    "# Convert it to a regular dictionary\n",
    "regular_dict_top5 = defaultdict_to_dict(global_accuracies)\n",
    "\n",
    "# Serialize and save to a file\n",
    "with open('accuracies_top5.pkl', 'wb') as file:\n",
    "    pickle.dump(regular_dict_top5, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e36e2d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': {'acc_train': 0.9996906897618311, 'acc_test': 0.9458807521029193}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('accuracies_top5.pkl', 'rb') as file:\n",
    "    loaded_dict = pickle.load(file)\n",
    "    # Optionally convert back to defaultdict\n",
    "    # my_defaultdict = convert_to_defaultdict(loaded_dict)\n",
    "    \n",
    "loaded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0fa5afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.9133473527956457\n"
     ]
    }
   ],
   "source": [
    "regular_dict_top1 = {}\n",
    "for group in model_managers:\n",
    "    df_temp = model_managers[group].test_df.copy()\n",
    "    df_temp['generated_doc_id_1beam'] = df_temp['generated_doc_id'].apply(lambda x: [x[0]])\n",
    "    regular_dict_top1[group] = {'acc_test':df_temp.apply(lambda row: row['doc_id'] in row['generated_doc_id_1beam'], \n",
    "                                axis=1).sum() / df_temp.shape[0]}\n",
    "    print (group, df_temp.apply(lambda row: row['doc_id'] in row['generated_doc_id_1beam'], \n",
    "                                axis=1).sum() / df_temp.shape[0])\n",
    "    \n",
    "    \n",
    "# display(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3f8d183",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('accuracies_top1.pkl', 'wb') as file:\n",
    "    pickle.dump(regular_dict_top1, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09770283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': {'acc_test': 0.9133473527956457}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('accuracies_top1.pkl', 'rb') as file:\n",
    "    loaded_dict = pickle.load(file)\n",
    "    # Optionally convert back to defaultdict\n",
    "    # my_defaultdict = convert_to_defaultdict(loaded_dict)\n",
    "    \n",
    "loaded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761341bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
